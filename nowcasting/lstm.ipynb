{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (FRED-MD)\n",
    "df = pd.read_csv(\n",
    "    '/home/js/macroeconvue/nowcasting/current.csv',\n",
    "    index_col='sasdate'\n",
    "    )\n",
    "# Drop target variable (CPIAUCSL)\n",
    "target = df['CPIAUCSL']\n",
    "df = df.drop(columns=['CPIAUCSL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the series stationary\n",
    "def transform_series(series, code):\n",
    "    if code == 1:\n",
    "        return series  # No transformation\n",
    "    elif code == 2:\n",
    "        return series.diff().dropna()  # First difference\n",
    "    elif code == 3:\n",
    "        return series.diff().diff().dropna()  # Second difference\n",
    "    elif code == 4:\n",
    "        return np.log(series).dropna()  # Logarithm\n",
    "    elif code == 5:\n",
    "        return np.log(series).diff().dropna()  # First difference of logarithm\n",
    "    elif code == 6:\n",
    "        return np.log(series).diff().diff().dropna()  # Second difference of logarithm\n",
    "    elif code == 7:\n",
    "        return series.pct_change().dropna()  # Percentage change\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown transformation code: {code}\")\n",
    "\n",
    "transformed_data = {}\n",
    "transformation_codes = df.iloc[0]  # Assuming the first row contains the codes\n",
    "data = df.iloc[1:]  # The actual data starts from the second row\n",
    "\n",
    "for column in data.columns:\n",
    "    code = transformation_codes[column]\n",
    "    transformed_data[column] = transform_series(data[column], code)\n",
    "\n",
    "df = pd.DataFrame(transformed_data).dropna(how='all')  # Drop rows with all NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "df = pd.DataFrame(StandardScaler().fit_transform(df), columns=df.columns, index=df.index)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA fto look for the number of components to retain\n",
    "from sklearn.decomposition import PCA\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# pca = PCA()\n",
    "# pca.fit(df)\n",
    "\n",
    "# cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# # Plot cumulative explained variance\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(cumulative_variance, marker='o', linestyle='--')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('Explained Variance by Number of Components')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA len: 390, Original: 390\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>...</th>\n",
       "      <th>PC31</th>\n",
       "      <th>PC32</th>\n",
       "      <th>PC33</th>\n",
       "      <th>PC34</th>\n",
       "      <th>PC35</th>\n",
       "      <th>PC36</th>\n",
       "      <th>PC37</th>\n",
       "      <th>PC38</th>\n",
       "      <th>PC39</th>\n",
       "      <th>PC40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RPI</th>\n",
       "      <td>0.034481</td>\n",
       "      <td>-0.081739</td>\n",
       "      <td>-0.068953</td>\n",
       "      <td>-0.092153</td>\n",
       "      <td>0.009781</td>\n",
       "      <td>-0.273636</td>\n",
       "      <td>-0.289818</td>\n",
       "      <td>0.398063</td>\n",
       "      <td>-0.132497</td>\n",
       "      <td>-0.177616</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071687</td>\n",
       "      <td>-0.005041</td>\n",
       "      <td>-0.003556</td>\n",
       "      <td>-0.051049</td>\n",
       "      <td>0.008884</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>-0.054323</td>\n",
       "      <td>0.024239</td>\n",
       "      <td>0.049147</td>\n",
       "      <td>-0.039312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W875RX1</th>\n",
       "      <td>0.043448</td>\n",
       "      <td>-0.117144</td>\n",
       "      <td>-0.074420</td>\n",
       "      <td>-0.049564</td>\n",
       "      <td>-0.085013</td>\n",
       "      <td>-0.106425</td>\n",
       "      <td>-0.033869</td>\n",
       "      <td>0.130573</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>0.036071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309834</td>\n",
       "      <td>-0.047977</td>\n",
       "      <td>0.351154</td>\n",
       "      <td>-0.098933</td>\n",
       "      <td>0.179590</td>\n",
       "      <td>-0.024453</td>\n",
       "      <td>0.067097</td>\n",
       "      <td>-0.105211</td>\n",
       "      <td>-0.143090</td>\n",
       "      <td>-0.060521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DPCERA3M086SBEA</th>\n",
       "      <td>0.122792</td>\n",
       "      <td>-0.063226</td>\n",
       "      <td>-0.098465</td>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.023704</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>-0.070860</td>\n",
       "      <td>0.054086</td>\n",
       "      <td>-0.045489</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099707</td>\n",
       "      <td>-0.034419</td>\n",
       "      <td>-0.024622</td>\n",
       "      <td>-0.045716</td>\n",
       "      <td>-0.109427</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.148523</td>\n",
       "      <td>-0.014004</td>\n",
       "      <td>-0.093088</td>\n",
       "      <td>0.080141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CMRMTSPLx</th>\n",
       "      <td>0.096000</td>\n",
       "      <td>-0.072448</td>\n",
       "      <td>-0.100212</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>-0.036803</td>\n",
       "      <td>-0.002458</td>\n",
       "      <td>0.029059</td>\n",
       "      <td>-0.040573</td>\n",
       "      <td>-0.023109</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081770</td>\n",
       "      <td>-0.010774</td>\n",
       "      <td>-0.014119</td>\n",
       "      <td>-0.009034</td>\n",
       "      <td>-0.097109</td>\n",
       "      <td>0.098720</td>\n",
       "      <td>0.032700</td>\n",
       "      <td>-0.078907</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>0.065193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RETAILx</th>\n",
       "      <td>0.140828</td>\n",
       "      <td>-0.029037</td>\n",
       "      <td>-0.079120</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.015920</td>\n",
       "      <td>-0.001387</td>\n",
       "      <td>-0.103307</td>\n",
       "      <td>0.101696</td>\n",
       "      <td>-0.048474</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.087943</td>\n",
       "      <td>-0.050115</td>\n",
       "      <td>-0.028114</td>\n",
       "      <td>-0.079790</td>\n",
       "      <td>-0.083041</td>\n",
       "      <td>0.074133</td>\n",
       "      <td>0.169641</td>\n",
       "      <td>-0.062955</td>\n",
       "      <td>-0.105989</td>\n",
       "      <td>0.124353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UMCSENTx</th>\n",
       "      <td>0.031067</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>-0.044416</td>\n",
       "      <td>0.028852</td>\n",
       "      <td>-0.058411</td>\n",
       "      <td>0.114273</td>\n",
       "      <td>-0.151772</td>\n",
       "      <td>0.091813</td>\n",
       "      <td>0.027816</td>\n",
       "      <td>0.097119</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362074</td>\n",
       "      <td>-0.027771</td>\n",
       "      <td>-0.287264</td>\n",
       "      <td>0.412850</td>\n",
       "      <td>-0.305325</td>\n",
       "      <td>0.155843</td>\n",
       "      <td>-0.113642</td>\n",
       "      <td>0.073663</td>\n",
       "      <td>0.144311</td>\n",
       "      <td>0.235216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTCOLNVHFNM</th>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.005823</td>\n",
       "      <td>-0.016231</td>\n",
       "      <td>0.074108</td>\n",
       "      <td>0.210359</td>\n",
       "      <td>0.232328</td>\n",
       "      <td>0.152782</td>\n",
       "      <td>0.341158</td>\n",
       "      <td>0.016444</td>\n",
       "      <td>-0.081195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063754</td>\n",
       "      <td>-0.017200</td>\n",
       "      <td>0.029651</td>\n",
       "      <td>0.112404</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>-0.217275</td>\n",
       "      <td>0.207222</td>\n",
       "      <td>-0.080242</td>\n",
       "      <td>0.396246</td>\n",
       "      <td>-0.266723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DTCTHFNM</th>\n",
       "      <td>0.016773</td>\n",
       "      <td>0.009038</td>\n",
       "      <td>-0.007257</td>\n",
       "      <td>0.021044</td>\n",
       "      <td>0.276215</td>\n",
       "      <td>0.248237</td>\n",
       "      <td>0.246983</td>\n",
       "      <td>0.447468</td>\n",
       "      <td>0.124558</td>\n",
       "      <td>-0.032936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039265</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>-0.064861</td>\n",
       "      <td>0.029045</td>\n",
       "      <td>0.085820</td>\n",
       "      <td>-0.091886</td>\n",
       "      <td>0.086665</td>\n",
       "      <td>-0.098595</td>\n",
       "      <td>0.124691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVEST</th>\n",
       "      <td>0.011748</td>\n",
       "      <td>0.018594</td>\n",
       "      <td>0.004939</td>\n",
       "      <td>-0.006219</td>\n",
       "      <td>0.003708</td>\n",
       "      <td>-0.179617</td>\n",
       "      <td>0.026435</td>\n",
       "      <td>-0.033200</td>\n",
       "      <td>0.013989</td>\n",
       "      <td>0.133150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.138504</td>\n",
       "      <td>-0.079306</td>\n",
       "      <td>0.125632</td>\n",
       "      <td>-0.197202</td>\n",
       "      <td>-0.239922</td>\n",
       "      <td>-0.208509</td>\n",
       "      <td>0.214371</td>\n",
       "      <td>0.063920</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>-0.044721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VIXCLSx</th>\n",
       "      <td>-0.105142</td>\n",
       "      <td>0.066829</td>\n",
       "      <td>-0.010686</td>\n",
       "      <td>0.137264</td>\n",
       "      <td>0.213191</td>\n",
       "      <td>-0.156890</td>\n",
       "      <td>-0.016009</td>\n",
       "      <td>-0.091678</td>\n",
       "      <td>-0.014785</td>\n",
       "      <td>-0.165004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202131</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>-0.072635</td>\n",
       "      <td>-0.328966</td>\n",
       "      <td>-0.023981</td>\n",
       "      <td>0.048349</td>\n",
       "      <td>-0.079471</td>\n",
       "      <td>0.218156</td>\n",
       "      <td>0.279308</td>\n",
       "      <td>0.003030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      PC1       PC2       PC3       PC4       PC5       PC6  \\\n",
       "RPI              0.034481 -0.081739 -0.068953 -0.092153  0.009781 -0.273636   \n",
       "W875RX1          0.043448 -0.117144 -0.074420 -0.049564 -0.085013 -0.106425   \n",
       "DPCERA3M086SBEA  0.122792 -0.063226 -0.098465  0.045354  0.023704  0.010595   \n",
       "CMRMTSPLx        0.096000 -0.072448 -0.100212  0.037263  0.001539 -0.036803   \n",
       "RETAILx          0.140828 -0.029037 -0.079120  0.006845  0.015920 -0.001387   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "UMCSENTx         0.031067 -0.006650 -0.044416  0.028852 -0.058411  0.114273   \n",
       "DTCOLNVHFNM      0.016934  0.005823 -0.016231  0.074108  0.210359  0.232328   \n",
       "DTCTHFNM         0.016773  0.009038 -0.007257  0.021044  0.276215  0.248237   \n",
       "INVEST           0.011748  0.018594  0.004939 -0.006219  0.003708 -0.179617   \n",
       "VIXCLSx         -0.105142  0.066829 -0.010686  0.137264  0.213191 -0.156890   \n",
       "\n",
       "                      PC7       PC8       PC9      PC10  ...      PC31  \\\n",
       "RPI             -0.289818  0.398063 -0.132497 -0.177616  ... -0.071687   \n",
       "W875RX1         -0.033869  0.130573 -0.013163  0.036071  ...  0.309834   \n",
       "DPCERA3M086SBEA -0.070860  0.054086 -0.045489  0.000375  ... -0.099707   \n",
       "CMRMTSPLx       -0.002458  0.029059 -0.040573 -0.023109  ... -0.081770   \n",
       "RETAILx         -0.103307  0.101696 -0.048474 -0.019287  ... -0.087943   \n",
       "...                   ...       ...       ...       ...  ...       ...   \n",
       "UMCSENTx        -0.151772  0.091813  0.027816  0.097119  ...  0.362074   \n",
       "DTCOLNVHFNM      0.152782  0.341158  0.016444 -0.081195  ...  0.063754   \n",
       "DTCTHFNM         0.246983  0.447468  0.124558 -0.032936  ... -0.039265   \n",
       "INVEST           0.026435 -0.033200  0.013989  0.133150  ... -0.138504   \n",
       "VIXCLSx         -0.016009 -0.091678 -0.014785 -0.165004  ...  0.202131   \n",
       "\n",
       "                     PC32      PC33      PC34      PC35      PC36      PC37  \\\n",
       "RPI             -0.005041 -0.003556 -0.051049  0.008884  0.072719 -0.054323   \n",
       "W875RX1         -0.047977  0.351154 -0.098933  0.179590 -0.024453  0.067097   \n",
       "DPCERA3M086SBEA -0.034419 -0.024622 -0.045716 -0.109427  0.004947  0.148523   \n",
       "CMRMTSPLx       -0.010774 -0.014119 -0.009034 -0.097109  0.098720  0.032700   \n",
       "RETAILx         -0.050115 -0.028114 -0.079790 -0.083041  0.074133  0.169641   \n",
       "...                   ...       ...       ...       ...       ...       ...   \n",
       "UMCSENTx        -0.027771 -0.287264  0.412850 -0.305325  0.155843 -0.113642   \n",
       "DTCOLNVHFNM     -0.017200  0.029651  0.112404  0.019135 -0.217275  0.207222   \n",
       "DTCTHFNM         0.031558  0.001580 -0.064861  0.029045  0.085820 -0.091886   \n",
       "INVEST          -0.079306  0.125632 -0.197202 -0.239922 -0.208509  0.214371   \n",
       "VIXCLSx          0.004698 -0.072635 -0.328966 -0.023981  0.048349 -0.079471   \n",
       "\n",
       "                     PC38      PC39      PC40  \n",
       "RPI              0.024239  0.049147 -0.039312  \n",
       "W875RX1         -0.105211 -0.143090 -0.060521  \n",
       "DPCERA3M086SBEA -0.014004 -0.093088  0.080141  \n",
       "CMRMTSPLx       -0.078907  0.003814  0.065193  \n",
       "RETAILx         -0.062955 -0.105989  0.124353  \n",
       "...                   ...       ...       ...  \n",
       "UMCSENTx         0.073663  0.144311  0.235216  \n",
       "DTCOLNVHFNM     -0.080242  0.396246 -0.266723  \n",
       "DTCTHFNM         0.086665 -0.098595  0.124691  \n",
       "INVEST           0.063920  0.002685 -0.044721  \n",
       "VIXCLSx          0.218156  0.279308  0.003030  \n",
       "\n",
       "[125 rows x 40 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply PCA to keep 90% of variance\n",
    "pca = PCA(n_components=0.90)\n",
    "pca.fit(df)\n",
    "data = pca.transform(df)\n",
    "print(f\"PCA len: {len(data)}, Original: {len(df)}\")\n",
    "\n",
    "# Look at relationship between original features nad cleanred_df\n",
    "loadings = pca.components_\n",
    "loadings_df = pd.DataFrame(loadings.T, index=df.columns, columns=[f'PC{i+1}' for i in range(loadings.shape[0])])\n",
    "display(loadings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for the LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Convert data into sequences\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:i+time_steps])\n",
    "        ys.append(y[i+time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 18  # Choose based on your data\n",
    "X, y = create_sequences(data, target.values, time_steps)\n",
    "\n",
    "# Split into train and test\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/js/macroeconvue/nowcasting/.venv/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - loss: 3286.7034 - val_loss: 11902.2529\n",
      "Epoch 2/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1668.0173 - val_loss: 8287.5195\n",
      "Epoch 3/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1011.6348 - val_loss: 4462.7744\n",
      "Epoch 4/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 540.2462 - val_loss: 4487.9814\n",
      "Epoch 5/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -14548us/step - loss: 447.7654 - val_loss: 5672.5391\n",
      "Epoch 6/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 377.7816 - val_loss: 5841.7520\n",
      "Epoch 7/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 265.2877 - val_loss: 6000.6836\n",
      "Epoch 8/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 327.9610 - val_loss: 5252.5830\n",
      "Epoch 9/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - loss: 215.9785 - val_loss: 5146.4092\n",
      "Epoch 10/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 208.1369 - val_loss: 4823.3989\n",
      "Epoch 11/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 130.7633 - val_loss: 5492.5000\n",
      "Epoch 12/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 126.8962 - val_loss: 5010.8252\n",
      "Epoch 13/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 90.0672 - val_loss: 5062.2720\n",
      "Epoch 14/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 72.5143 - val_loss: 4849.8394\n",
      "Epoch 15/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 73.6784 - val_loss: 4599.7910\n",
      "Epoch 16/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 86.7304 - val_loss: 5138.1392\n",
      "Epoch 17/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 82.8977 - val_loss: 5276.7119\n",
      "Epoch 18/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 63.4492 - val_loss: 5363.0225\n",
      "Epoch 19/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 68.8171 - val_loss: 4906.6084\n",
      "Epoch 20/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-0s\u001b[0m -24038us/step - loss: 72.4033 - val_loss: 5177.4717\n",
      "Epoch 21/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 71.1710 - val_loss: 5703.7358\n",
      "Epoch 22/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 100.9524 - val_loss: 4880.8032\n",
      "Epoch 23/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 88.5255 - val_loss: 4011.2659\n",
      "Epoch 24/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 118.9183 - val_loss: 4643.3252\n",
      "Epoch 25/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 89.5147 - val_loss: 4826.7373\n",
      "Epoch 26/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 67.8796 - val_loss: 4951.0771\n",
      "Epoch 27/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 39.3487 - val_loss: 4759.0923\n",
      "Epoch 28/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 59.2371 - val_loss: 4649.9780\n",
      "Epoch 29/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 46.3413 - val_loss: 4664.8384\n",
      "Epoch 30/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 62.5065 - val_loss: 5224.2896\n",
      "Epoch 31/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 70.5863 - val_loss: 4709.6909\n",
      "Epoch 32/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 75.6565 - val_loss: 4567.7173\n",
      "Epoch 33/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 39.0863 - val_loss: 4512.3535\n",
      "Epoch 34/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 45.7048 - val_loss: 5160.0566\n",
      "Epoch 35/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 55.6324 - val_loss: 5081.8003\n",
      "Epoch 36/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 51.7926 - val_loss: 5135.7578\n",
      "Epoch 37/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 44.5227 - val_loss: 5260.5938\n",
      "Epoch 38/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 48.5817 - val_loss: 4928.7168\n",
      "Epoch 39/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 658ms/step - loss: 35.0995 - val_loss: 4799.7529\n",
      "Epoch 40/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-6s\u001b[0m 25ms/step - loss: 44.9661 - val_loss: 4996.2783\n",
      "Epoch 41/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 58.0776 - val_loss: 4690.1235\n",
      "Epoch 42/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 60.4800 - val_loss: 4495.1631\n",
      "Epoch 43/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 55.8705 - val_loss: 4919.4331\n",
      "Epoch 44/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 54.0485 - val_loss: 4951.2510\n",
      "Epoch 45/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 37.5229 - val_loss: 5190.1943\n",
      "Epoch 46/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 50.6799 - val_loss: 5190.6621\n",
      "Epoch 47/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 41.5016 - val_loss: 5015.2617\n",
      "Epoch 48/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 37.7252 - val_loss: 4833.2158\n",
      "Epoch 49/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 29.2293 - val_loss: 5039.1426\n",
      "Epoch 50/50\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 39.2519 - val_loss: 4708.3340\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, activation='relu', return_sequences=False, kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.2),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 67.09467274807682\n",
      "Test MSE: 4706.87229854542\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "train_mse = mean_squared_error(y_train, train_predict)\n",
    "test_mse = mean_squared_error(y_test, test_predict)\n",
    "print(f\"Train MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "# Inverse transform to get actual values\n",
    "# train_predict = target.iloc[:split].values + train_predict.flatten()\n",
    "# test_predict = target.iloc[split:].values + test_predict.flatten()\n",
    "# # Plot the results\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(14, 7)) \n",
    "# plt.plot(target.index[:split], target.iloc[:split], label='Train Actual', color='blue')\n",
    "# plt.plot(target.index[split:], target.iloc[split:], label='Test Actual', color='orange')\n",
    "# plt.plot(target.index[:split], train_predict, label='Train Predicted', color='green')\n",
    "# plt.plot(target.index[split:], test_predict, label='Test Predicted', color='red')\n",
    "# plt.title('LSTM Model Predictions vs Actual')\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('CPIAUCSL')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
